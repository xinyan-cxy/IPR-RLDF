
"""
Use the GLIP model to process images. 
For each image-prompt pair, the pretrained GLIP model will generate bounding boxes on the image.
Save the images generated by GLIP.
"""
from PIL import Image
import numpy as np
from maskrcnn_benchmark.config import cfg
# pip install nltk opencv-python inflect scipy pycocotools
from maskrcnn_benchmark.engine.predictor_glip import GLIPDemo
import os
import json
import argparse
import random
from tqdm import tqdm

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--iter",
        default=0,
        help="num of iter",
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=4,
        help="how many samples to produce for each given prompt.",
    )
    parser.add_argument(
        "--thresh",
        default=0.45,
        help="thresh of glip score",
    )
    parser.add_argument(
        "--dir",
        default="../output",
        help="directory of detection results",
    )
    parser.add_argument(
        "--model",
        type=str,
        default="sdv2",
        help="base model (sdxl or sdv2)",
    )
    opt = parser.parse_args()
    return opt

def main(opt):   
    num = opt.iter
    directory = os.path.join(opt.dir, f"{opt.model}_dataset/{num}")
    target_directory = os.path.join(opt.dir, f"{opt.model}_dataset_new/{num}")
    os.makedirs(target_directory, exist_ok=True)
    # dump a dict of glip info
    glip_data = {}

    def load(path):
        """
        Given the path of an image, get the np array of this image
        """
        pil_image = Image.open(path).convert("RGB")
        # convert to BGR format
        image = np.array(pil_image) #[:, :, [2, 1, 0]]
        return image

    # GLPT-T model
    # config_file = "configs/pretrain/glip_Swin_T_O365_GoldG.yaml"
    # weight_file = "MODEL/glip_tiny_model_o365_goldg_cc_sbu.pth"

    # GLPT-L model
    # ! wget https://penzhanwu2bbs.blob.core.windows.net/data/GLIPv1_Open/models/glip_large_model.pth -O MODEL/glip_large_model.pth
    config_file = "../GLIP/configs/pretrain/glip_Swin_L.yaml"
    weight_file = "../GLIP/MODEL/glip_large_model.pth" # <- change this

    # update the config options with the config file
    # manual override some options
    cfg.local_rank = 0
    cfg.num_gpus = 1
    cfg.merge_from_file(config_file)
    cfg.merge_from_list(["MODEL.WEIGHT", weight_file])
    cfg.merge_from_list(["MODEL.DEVICE", "cuda"])
    cfg.merge_from_list(["MODEL.LANGUAGE_BACKBONE.TOKENIZER_TYPE", "google-bertbert-base-uncased"])

    glip_demo = GLIPDemo(
        cfg,
        min_image_size=800,
        confidence_threshold=0.7,
        show_mask_heatmaps=False
    )

    viz_ids = [
        19100, 24653, 29131, 8606, 17652, 6603, 26515, 22815, 7904, 6486, 26363, 
        22495, 18253, 12812, 20714, 29841, 23283, 29120, 23113, 810, 9942, 22356, 
        3792, 7257, 29971, 20086, 20727, 10321, 2084, 27141, 30955, 29633, 23544, 
        13352, 27244, 19973, 7646, 21186, 7366, 17831, 8001, 12373, 12046, 8966, 
        7264, 15896, 29727, 5257, 4254, 8754, 17066, 7170, 26186, 16226, 8341,
        10516, 25814, 887, 19792, 24514, 3937, 27667, 19794, 7335, 21865, 5416, 
        14686, 31510, 27552, 18714, 14405, 4381, 23780, 22884, 22461, 21636, 14555, 
        18915, 10811, 19134, 3344, 13642, 21645, 16896, 22927, 6431, 29065, 1824, 
        14972, 8963, 13984, 26053, 22416, 11271, 28697, 17604, 18051, 5015, 15407, 
        6465
        ]

    # open json file
    with open('../text_spatial_rel_phrases.json', 'r') as f:
        text_data = json.load(f)

    for uniq_id in tqdm(viz_ids):
        free_form_prompt = text_data[uniq_id]["text"]
        obj1 = text_data[uniq_id]["obj_1_attributes"][0]
        obj2 = text_data[uniq_id]["obj_2_attributes"][0]
        rel = text_data[uniq_id]["rel_type"]
        # print("UNIQ_ID: {}; \tTEXT: {}; \tOBJ-A: {}; \tOBJ-B: {}; \tREL: {}".format(uniq_id, free_form_prompt, obj1, obj2, rel))
        for i in range(opt.batch_size):
            image_id = str(uniq_id) + "_" + str(i)
            path = os.path.join(directory, f'{image_id}.png')
            image = load(path)
            result, top_predictions, name_labels, glip_boxes, glip_scores = glip_demo.run_on_web_image(image, free_form_prompt, opt.thresh, custom_entity = [obj1, obj2])
            output_path = os.path.join(target_directory, f'{image_id}.png')
            #save image
            im = Image.fromarray(result)
            im.save(output_path)
            #save information of the image
            glip_data[image_id]={
                    "unique_id": uniq_id, 
                    "image_id": image_id,
                    "object_names": name_labels,
                    "glip_scores": glip_scores, 
                    "glip_boxes": glip_boxes.tolist(),
                    "obj1": obj1,
                    "obj2": obj2,
                    "relation": rel,
                    "prompt": free_form_prompt
                }

    #save all information to json file
    output_dir = os.path.join(opt.dir, f"glip_all_metadata_{opt.model}")
    os.makedirs(output_dir, exist_ok=True)
    output_dir_glip = os.path.join(output_dir, "glip_dataset_metadata_epoch"+str(num)+".json")
    print(output_dir_glip)
    with open(output_dir_glip, 'w') as f:
        json.dump(glip_data, f, indent = 2)

if __name__ == "__main__":
    opt = parse_args()
    main(opt)
